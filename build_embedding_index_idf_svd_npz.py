#!/usr/bin/env python3
"""
build_embedding_index_idf_svd_npz.py

Permanent serialization fix:
  - Writes/uses a companion importable module: idf_svd_components.py
  - The joblib model will reference idf_svd_components.PreprocessTransformer /
    idf_svd_components.AdaptiveTruncatedSVD instead of __main__.*,
    so joblib.load works from any downstream script (as long as the project
    directory is on sys.path; typically true when you run scripts from this project).

Compute IDF-SVD (TF-IDF -> TruncatedSVD/LSA -> optional L2 normalization) embeddings
for RIS sentences and save:

  1) embedding_index_idf_svd.npz (compact index bundle)
      - sentence_id: (N,) int64
      - embeddings:  (N, d) float array (dtype configurable; default preserves upstream precision)
      - created_at_utc, corpus_path, dataset_fingerprint_sha256, metadata_json

  2) idf_svd_model.joblib (fitted model for retrieval)
      - sklearn Pipeline:
          PreprocessTransformer  (from idf_svd_components)
          -> FeatureUnion(TFIDF word + optional char)
          -> AdaptiveTruncatedSVD (clips n_components safely; from idf_svd_components)
          -> Normalizer(optional)

Dependencies:
  python -m pip install -U scikit-learn scipy pyarrow joblib

Run:
  python build_embedding_index_idf_svd_npz.py \
  --corpus ris_sentences.parquet \
  --out_npz embedding_index_idf_svd.npz \
  --out_model idf_svd_model.joblib \
  --number_mode keep \
  --min_df_word 1 --max_df_word 0.99 \
  --min_df_char 1 --max_df_char 0.99 \
  --word_ngrams 1,3 --char_ngrams 3,6 \
  --max_features_word 600000 --max_features_char 300000 \
  --transformer_weight_word 1.0 --transformer_weight_char 1.0 \
  --dim 1024 --svd_n_iter 20 \
  --tfidf_dtype float32 --embeddings_dtype float32 \
  --union_n_jobs 1 \
  --no_l2_n_jobs 1 \
  --no_l2
"""

from __future__ import annotations

import argparse
import hashlib
import json
import os
import sys
import tempfile
import time
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Optional, Tuple, cast

import numpy as np
import pandas as pd


# ----------------------------- Defaults -----------------------------

DEFAULT_CORPUS_PATH = Path("ris_sentences.parquet")
DEFAULT_OUT_NPZ = Path("embedding_index_idf_svd.npz")
DEFAULT_OUT_MODEL = Path("idf_svd_model.joblib")

DEFAULT_ID_COL = "sentence_id"
DEFAULT_TEXT_COL = "sentence"

# NOTE: Silhouette-based tuning was removed because it did not improve downstream
# performance in evaluation. The pipeline now uses fixed settings provided via
# CLI/config (SVD dimension and char n-grams on/off).

# ----------------------------- Permanent fix: write/import components module -----------------------------

_COMPONENTS_MODULE_NAME = "idf_svd_components"
_COMPONENTS_VERSION = "3"

_COMPONENTS_SOURCE = r'''# Auto-generated by build_embedding_index_idf_svd_npz.py
# Do not edit unless you know what you're doing.
__COMPONENTS_VERSION__ = "__COMPONENTS_VERSION_PLACEHOLDER__"

import re
from typing import Optional

import numpy as np

try:
    from sklearn.base import BaseEstimator, TransformerMixin
except Exception:
    BaseEstimator = object
    TransformerMixin = object


_PARAGRAPH_RE = re.compile(r"ยง+")
_DIGIT_TOKEN_RE = re.compile(r"\b\d+\b")
_MULTI_SPACE_RE = re.compile(r"\s+")


def preprocess_legal_german(text: str, *, number_mode: str = "replace", number_prefix: str = "num") -> str:
    """
    Pragmatic preprocessing for German legal sentences.
    - Normalize paragraph signs to a stable token ("paragraf").
    - Handle standalone numeric tokens (default: replace with a stable token like "num12" instead of deleting).
    - Lowercase and normalize whitespace.

    number_mode:
      - "replace": replace standalone numbers with f"{number_prefix}<value>" (recommended for legal/citation retrieval)
      - "keep": keep numbers as-is (captured by char n-grams; word token_pattern may drop pure digits)
      - "remove": remove standalone numeric tokens (legacy behavior; generally not recommended for legal text)
    """
    s = str(text)
    s = _PARAGRAPH_RE.sub(" paragraf ", s)

    mode = (number_mode or "replace").strip().lower()
    if mode == "remove":
        s = _DIGIT_TOKEN_RE.sub(" ", s)
    elif mode == "replace":
        prefix = (number_prefix or "num").strip()
        if not prefix:
            prefix = "num"
        s = _DIGIT_TOKEN_RE.sub(lambda m: f" {prefix}{m.group(0)} ", s)
    elif mode == "keep":
        pass
    else:
        raise ValueError(f"Unsupported number_mode: {number_mode!r} (expected 'replace', 'keep', or 'remove').")

    s = s.lower()
    s = _MULTI_SPACE_RE.sub(" ", s).strip()
    return s


class PreprocessTransformer(BaseEstimator, TransformerMixin):
    """
    scikit-learn compatible transformer applying preprocess_legal_german.
    Top-level module class => joblib/pickle stable import path.
    """

    def __init__(self, number_mode: str = "replace", number_prefix: str = "num"):
        self.number_mode = number_mode
        self.number_prefix = number_prefix

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [
            preprocess_legal_german(x, number_mode=self.number_mode, number_prefix=self.number_prefix)
            for x in X
        ]


class AdaptiveTruncatedSVD(BaseEstimator, TransformerMixin):
    """
    TruncatedSVD wrapper that clips n_components at fit-time based on X shape.
    Prevents crashes if requested dim > (n_features-1) or (n_samples-1).

    Exposes:
      - n_components_ (effective)
      - explained_variance_ratio_
      - components_ (from underlying TruncatedSVD)
    """

    def __init__(
        self,
        n_components: int = 1024,
        *,
        n_iter: int = 20,
        random_state: int = 42,
        algorithm: str = "randomized",
    ):
        self.n_components = int(n_components)
        self.n_iter = int(n_iter)
        self.random_state = int(random_state)
        self.algorithm = str(algorithm)

        self.n_components_: Optional[int] = None
        self.model_ = None
        self.explained_variance_ratio_ = None
        self.components_ = None

    def fit(self, X, y=None):
        from sklearn.decomposition import TruncatedSVD

        n_samples = int(getattr(X, "shape", [0, 0])[0])
        n_features = int(getattr(X, "shape", [0, 0])[1])

        # Practical constraint: <= min(n_samples, n_features) - 1
        max_allowed = min(n_samples, n_features) - 1
        if max_allowed < 1:
            raise ValueError("Cannot fit TruncatedSVD: shape=({},{})".format(n_samples, n_features))

        n_comp = min(int(self.n_components), int(max_allowed))

        self.n_components_ = int(n_comp)
        self.model_ = TruncatedSVD(
            n_components=self.n_components_,
            algorithm=self.algorithm,
            n_iter=self.n_iter,
            random_state=self.random_state,
        )
        self.model_.fit(X)

        self.explained_variance_ratio_ = getattr(self.model_, "explained_variance_ratio_", None)
        self.components_ = getattr(self.model_, "components_", None)
        return self

    def transform(self, X):
        if self.model_ is None:
            raise RuntimeError("AdaptiveTruncatedSVD is not fitted.")
        return self.model_.transform(X)

    def fit_transform(self, X, y=None):
        return self.fit(X, y=y).transform(X)
'''
_COMPONENTS_SOURCE = _COMPONENTS_SOURCE.replace("__COMPONENTS_VERSION_PLACEHOLDER__", _COMPONENTS_VERSION)




def _atomic_write_text(path: Path, text: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path: Optional[Path] = None
    try:
        with tempfile.NamedTemporaryFile(
            mode="w",
            encoding="utf-8",
            dir=str(path.parent),
            prefix=path.stem + ".tmp.",
            suffix=path.suffix if path.suffix else ".py",
            delete=False,
        ) as f:
            tmp_path = Path(f.name)
            f.write(text)
        os.replace(tmp_path, path)
    except Exception:
        if tmp_path is not None and tmp_path.exists():
            try:
                tmp_path.unlink()
            except Exception:
                pass
        raise


def ensure_components_module(script_dir: Path) -> Path:
    """
    Ensure idf_svd_components.py exists (and matches expected version).
    Returns path to the module file.
    """
    mod_path = script_dir / f"{_COMPONENTS_MODULE_NAME}.py"
    try:
        existing = mod_path.read_text(encoding="utf-8", errors="replace") if mod_path.exists() else None
    except Exception:
        existing = None

    # Rewrite whenever content differs. This prevents stale modules when the embedded source changes.
    needs_write = (existing != _COMPONENTS_SOURCE)

    if needs_write:
        _atomic_write_text(mod_path, _COMPONENTS_SOURCE)

    return mod_path


def import_components(script_dir: Path):
    """
    Import idf_svd_components from script_dir (ensures stable pickle module path).
    """
    # Ensure script_dir is importable for downstream loads as well.
    if str(script_dir) not in sys.path:
        sys.path.insert(0, str(script_dir))

    import importlib

    mod = importlib.import_module(_COMPONENTS_MODULE_NAME)

    # Basic sanity check
    ver = getattr(mod, "__COMPONENTS_VERSION__", None)
    if ver != _COMPONENTS_VERSION:
        raise RuntimeError(
            f"Imported {_COMPONENTS_MODULE_NAME} version {ver}, expected {_COMPONENTS_VERSION}. "
            f"Delete {_COMPONENTS_MODULE_NAME}.py and rerun the builder."
        )
    return mod


# ----------------------------- Integrity fingerprint -----------------------------

def compute_dataset_fingerprint(
    df: pd.DataFrame,
    *,
    id_col: str,
    text_col: str,
    include_cols: Tuple[str, ...] = (),
) -> str:
    """
    Compute a stable SHA256 over (sentence_id + content) for all rows in order.

    This is intentionally equivalent to sha256_fingerprint_sentence_id_content:
      - hashes ONLY id_col and text_col
      - uses str(...) conversion for both fields
      - separators: NUL between id/text, newline per row
      - utf-8 encoding with strict error handling
    """
    if include_cols:
        raise ValueError(
            "This fingerprint function is defined to be equivalent to "
            "sha256_fingerprint_sentence_id_content and therefore does not "
            "support include_cols. Pass include_cols=() to proceed."
        )

    if id_col not in df.columns or text_col not in df.columns:
        raise ValueError(
            f"Corpus is missing required column '{id_col}' or '{text_col}'. "
            f"Columns: {list(df.columns)}"
        )

    h = hashlib.sha256()
    for sid, content in df[[id_col, text_col]].itertuples(index=False, name=None):
        h.update(str(sid).encode("utf-8"))
        h.update(b"\0")
        h.update(str(content).encode("utf-8"))
        h.update(b"\n")

    return h.hexdigest()


# ----------------------------- Atomic writers -----------------------------

def atomic_save_npz(out_path: Path, *, compressed: bool, verify: bool = True, **payload: Any) -> None:
    """
    Atomically write NPZ: write temp *.npz in same dir, verify, then os.replace().
    Uses a file handle so NumPy does not append suffix unexpectedly.
    """
    out_path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path: Optional[Path] = None
    try:
        with tempfile.NamedTemporaryFile(
            mode="wb",
            dir=str(out_path.parent),
            prefix=out_path.stem + ".tmp.",
            suffix=".npz",
            delete=False,
        ) as f:
            tmp_path = Path(f.name)
            if compressed:
                np.savez_compressed(f, **payload)
            else:
                np.savez(f, **payload)

        if verify:
            # IMPORTANT (Windows): np.load keeps the file handle open until closed.
            # Using a context manager prevents WinError 32 on os.replace().
            with np.load(tmp_path, allow_pickle=False) as d:
                if "sentence_id" not in d or "embeddings" not in d:
                    raise RuntimeError("NPZ verification failed: missing required keys.")
                if d["sentence_id"].ndim != 1 or d["embeddings"].ndim != 2:
                    raise RuntimeError("NPZ verification failed: unexpected shapes.")

        os.replace(tmp_path, out_path)
    except Exception:
        if tmp_path is not None and tmp_path.exists():
            try:
                tmp_path.unlink()
            except Exception:
                pass
        raise


def atomic_joblib_dump(obj: Any, out_path: Path, *, compress: int = 3) -> None:
    """
    Atomically write a joblib file: dump to temp file in same dir, then os.replace().
    """
    import joblib

    out_path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path: Optional[Path] = None
    try:
        with tempfile.NamedTemporaryFile(
            mode="wb",
            dir=str(out_path.parent),
            prefix=out_path.stem + ".tmp.",
            suffix=out_path.suffix if out_path.suffix else ".joblib",
            delete=False,
        ) as f:
            tmp_path = Path(f.name)

        joblib.dump(obj, str(tmp_path), compress=compress)
        _ = joblib.load(str(tmp_path))  # verify it can be loaded

        os.replace(tmp_path, out_path)
    except Exception:
        if tmp_path is not None and tmp_path.exists():
            try:
                tmp_path.unlink()
            except Exception:
                pass
        raise


# ----------------------------- Config -----------------------------

@dataclass(frozen=True)
class IDFSVDConfig:
    # Vectorization
    word_ngram_range: Tuple[int, int] = (1, 3)
    char_ngram_range: Tuple[int, int] = (3, 6)
    use_char_ngrams: bool = True

    # Document frequency thresholds (separate for word/char for better control)
    min_df_word: int = 1
    max_df_word: float = 0.99
    min_df_char: int = 1
    max_df_char: float = 0.99

    max_features_word: int = 600_000
    max_features_char: int = 300_000

    # Branch weights for FeatureUnion (downweight char features by default)
    transformer_weight_word: float = 1.0
    transformer_weight_char: float = 1.0

    # Preprocessing
    number_mode: str = "keep"   # replace|keep|remove
    number_prefix: str = "num"

    sublinear_tf: bool = False

    # SVD
    dim: int = 1024
    svd_n_iter: int = 20
    random_state: int = 42

    # Postprocessing
    l2_normalize: bool = False


def build_idf_svd_pipeline(
    *,
    config: IDFSVDConfig,
    use_char_ngrams: bool,
    dim: int,
    components_mod,
    tfidf_dtype: Optional[np.dtype] = None,
    union_n_jobs: Optional[int] = None,
):
    """
    Returns a scikit-learn Pipeline for IDF-SVD embeddings.

    Notes
    - Uses classes from idf_svd_components (NOT from __main__), so joblib is portable.
    - By default, TF-IDF and downstream computations use scikit-learn defaults
      (typically float64). You may optionally request float32 via `tfidf_dtype`.
    - `union_n_jobs` parallelizes the word-vs-char branches in FeatureUnion.
    """
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.pipeline import Pipeline, FeatureUnion
    from sklearn.preprocessing import Normalizer
    from sklearn.base import TransformerMixin

    PreprocessTransformer = components_mod.PreprocessTransformer
    AdaptiveTruncatedSVD = components_mod.AdaptiveTruncatedSVD

    # Build vectorizers with optional dtype (to avoid forcing float32 by default).
    # NOTE: Type checkers infer a narrow union for `dict(...)` values here (str|tuple|int|float|None).
    # We intentionally allow heterogeneous values (including NumPy dtypes) in this param dict.
    word_params: Dict[str, Any] = dict(
        analyzer="word",
        ngram_range=config.word_ngram_range,
        min_df=config.min_df_word,
        max_df=config.max_df_word,
        max_features=config.max_features_word,
        sublinear_tf=config.sublinear_tf,
        lowercase=False,  # lowercase done in PreprocessTransformer
        norm=None,        # postpone normalization to after SVD
        token_pattern=r"(?u)\b[^\W\d_][^\W_]+\b",
    )
    if tfidf_dtype is not None:
        # sklearn's vectorizers expect a scalar *type* (e.g., np.float32), but np.dtype works at runtime.
        # Using `.type` is both correct and type-checker-friendly.
        word_params["dtype"] = getattr(tfidf_dtype, "type", tfidf_dtype)
    word_vectorizer = TfidfVectorizer(**word_params)

    transformer_list: list[tuple[str, TransformerMixin | Pipeline]] = [
        ("word_tfidf", cast(TransformerMixin, word_vectorizer)),
    ]

    if use_char_ngrams:
        char_params: Dict[str, Any] = dict(
            analyzer="char_wb",
            ngram_range=config.char_ngram_range,
            min_df=config.min_df_char,
            max_df=config.max_df_char,
            max_features=config.max_features_char,
            sublinear_tf=config.sublinear_tf,
            lowercase=False,
            norm=None,
        )
        if tfidf_dtype is not None:
            char_params["dtype"] = getattr(tfidf_dtype, "type", tfidf_dtype)
        char_vectorizer = TfidfVectorizer(**char_params)
        transformer_list.append(("char_tfidf", cast(TransformerMixin, char_vectorizer)))

    fu_kwargs = {}
    if union_n_jobs is not None:
        fu_kwargs["n_jobs"] = int(union_n_jobs)

    # Optional branch weighting: char n-grams often dominate variance; downweight by default.
    weights = {"word_tfidf": float(config.transformer_weight_word)}
    if use_char_ngrams:
        weights["char_tfidf"] = float(config.transformer_weight_char)
    fu_kwargs["transformer_weights"] = weights

    # Correct API: transformer_list (compat across scikit-learn versions)
    try:
        union = FeatureUnion(transformer_list=transformer_list, **fu_kwargs)
    except TypeError:
        # Some older sklearn versions don't support transformer_list kwarg and/or transformer_weights.
        fu_kwargs.pop("transformer_weights", None)
        try:
            union = FeatureUnion(transformer_list=transformer_list, **fu_kwargs)
        except TypeError:
            union = FeatureUnion(transformer_list, **fu_kwargs)


    steps = [
        ("preprocess", PreprocessTransformer(number_mode=config.number_mode, number_prefix=config.number_prefix)),
        ("tfidf_union", union),
        ("svd", AdaptiveTruncatedSVD(
            n_components=int(dim),
            algorithm="randomized",
            n_iter=config.svd_n_iter,
            random_state=config.random_state,
        )),
    ]

    if config.l2_normalize:
        steps.append(("l2norm", Normalizer(copy=False)))

    return Pipeline(steps=steps)



# ----------------------------- Builder (NPZ + model) -----------------------------

def build_embedding_index_idf_svd_npz_and_model(
    *,
    sentences_parquet_path: str | Path,
    out_npz_path: str | Path,
    out_model_path: str | Path,
    sentence_id_col: str = DEFAULT_ID_COL,
    text_col: str = DEFAULT_TEXT_COL,
    compressed_npz: bool = True,
    joblib_compress: int = 3,
    compute_fingerprint_flag: bool = True,
    config: IDFSVDConfig = IDFSVDConfig(),

    # Dtypes / parallelism
    tfidf_dtype: str = "float32",
    embeddings_dtype: str = "float32",
    union_n_jobs: int = 1,
) -> Dict[str, Any]:
    """
    Computes IDF-SVD embeddings, saves NPZ + fitted model (joblib).
    """
    import sklearn  # for metadata

    t0 = time.perf_counter()

    # ----------------------------- Numeric precision controls -----------------------------
    # By default, we do NOT downcast: scikit-learn defaults (typically float64) are preserved.
    tfidf_np_dtype = _resolve_optional_float_dtype(tfidf_dtype)
    emb_np_dtype = _resolve_optional_float_dtype(embeddings_dtype)

    sentences_parquet_path = Path(sentences_parquet_path)
    out_npz_path = Path(out_npz_path)
    out_model_path = Path(out_model_path)

    # Ensure and import components module (permanent pickle path)
    script_dir = Path(__file__).resolve().parent
    ensure_components_module(script_dir)
    components_mod = import_components(script_dir)
    # (Preprocess is applied inside the sklearn Pipeline.)

    print(f"Loading corpus: {sentences_parquet_path}")

    # Read only required columns up front to reduce I/O and memory footprint.
    try:
        df = pd.read_parquet(sentences_parquet_path, columns=[sentence_id_col, text_col])
    except TypeError:
        # Older pandas/engines may not support column projection here.
        df = pd.read_parquet(sentences_parquet_path)

    # If fingerprinting is enabled, opportunistically load the optional columns used by the fingerprint
    # without forcing a full-column read when not needed.
    if compute_fingerprint_flag:
        for c in ("law_type", "page"):
            if c in df.columns:
                continue
            try:
                extra = pd.read_parquet(sentences_parquet_path, columns=[c])
                if c in extra.columns and len(extra) == len(df):
                    df[c] = extra[c]
            except Exception:
                pass

    if sentence_id_col not in df.columns:
        raise ValueError(f"Missing '{sentence_id_col}' in parquet. Columns: {list(df.columns)}")
    if text_col not in df.columns:
        raise ValueError(f"Missing '{text_col}' in parquet. Columns: {list(df.columns)}")

    ids = df[sentence_id_col].astype(np.int64).to_numpy()
    if len(np.unique(ids)) != len(ids):
        raise ValueError(f"'{sentence_id_col}' must be unique (duplicates found).")

    texts_raw = df[text_col].astype(str).tolist()
    N = len(texts_raw)
    if N == 0:
        raise ValueError("No sentences found (N=0).")
    print(f"Number of sentences: {N:,}")

    fingerprint = ""
    if compute_fingerprint_flag:
        print("Computing dataset fingerprint (sha256) ...")
        fp_start = time.perf_counter()
        fingerprint = compute_dataset_fingerprint(df, id_col=sentence_id_col, text_col=text_col)
        fp_end = time.perf_counter()
        print(f"Fingerprint computed in {fp_end - fp_start:.1f} s: {fingerprint}")
    # Silhouette-based tuning removed (downstream performance did not improve).
    # We always use the fixed config/CLI settings.
    chosen_dim = int(config.dim)
    chosen_use_char = bool(config.use_char_ngrams)
    print(f"Using fixed settings: dim={chosen_dim}, use_char_ngrams={chosen_use_char}")

    # ----------------------------- Fit final pipeline on full corpus -----------------------------
    print("Fitting final IDF-SVD pipeline on full corpus ...")

    # FeatureUnion parallelism (parallelizes the word-vs-char branches).
    if union_n_jobs == 0:
        cpu = os.cpu_count() or 1
        union_n_jobs_eff = 2 if (chosen_use_char and cpu >= 2) else 1
    else:
        union_n_jobs_eff = int(union_n_jobs)
    union_n_jobs_eff = max(1, union_n_jobs_eff)
    union_n_jobs_eff = None if union_n_jobs_eff == 1 else union_n_jobs_eff

    pipeline = build_idf_svd_pipeline(config=config, use_char_ngrams=chosen_use_char, dim=chosen_dim, components_mod=components_mod, tfidf_dtype=tfidf_np_dtype, union_n_jobs=union_n_jobs_eff)
    Z = pipeline.fit_transform(texts_raw)
    Z = np.asarray(Z)
    if emb_np_dtype is not None and Z.dtype != emb_np_dtype:
        Z = Z.astype(emb_np_dtype, copy=False)

    if Z.ndim != 2 or Z.shape[0] != N:
        raise RuntimeError(f"Unexpected embedding matrix shape: {Z.shape} (expected ({N}, d))")

    d = int(Z.shape[1])
    print(f"Embeddings shape: {Z.shape} ({Z.dtype})")

    svd_step = pipeline.named_steps["svd"]
    evr = getattr(svd_step, "explained_variance_ratio_", None)
    explained_var_sum = float(np.sum(evr)) if evr is not None else 0.0
    effective_dim = int(getattr(svd_step, "n_components_", d) or d)

    # Attach metadata to pipeline (optional but useful)
    created_at = datetime.now(timezone.utc).isoformat()

    meta = {
        "created_at_utc": created_at,
        "method": "tfidf+svd (lsa)",
        "tfidf_dtype": (np.dtype(tfidf_np_dtype).name if tfidf_np_dtype is not None else "auto"),
        "embeddings_dtype": str(Z.dtype),
        "union_n_jobs": int(union_n_jobs_eff or 1),
        "chosen_dim_requested": int(chosen_dim),
        "chosen_dim_effective": int(effective_dim),
        "use_char_ngrams": bool(chosen_use_char),
        "vectorizer": {
            "word_ngram_range": list(config.word_ngram_range),
            "char_ngram_range": list(config.char_ngram_range),
            # Separate df thresholds for word vs char (char often benefits from different settings)
            "min_df_word": int(config.min_df_word),
            "max_df_word": float(config.max_df_word),
            "min_df_char": int(config.min_df_char),
            "max_df_char": float(config.max_df_char),
            # Legacy aliases (use with care if word/char thresholds differ)
            "min_df": int(config.min_df_word),
            "max_df": float(config.max_df_word),
            "transformer_weight_word": float(config.transformer_weight_word),
            "transformer_weight_char": float(config.transformer_weight_char),
            "number_mode": str(config.number_mode),
            "number_prefix": str(config.number_prefix),
            "max_features_word": config.max_features_word,
            "max_features_char": config.max_features_char,
            "sublinear_tf": config.sublinear_tf,
        },
        "postprocess": {"l2_normalize": bool(config.l2_normalize)},
        "svd": {"explained_variance_ratio_sum": explained_var_sum, "n_iter": config.svd_n_iter},
        "environment": {"numpy": np.__version__, "pandas": pd.__version__, "sklearn": sklearn.__version__},
        "dataset_fingerprint_sha256": fingerprint,
        "corpus_path": str(sentences_parquet_path),
        "components_module": _COMPONENTS_MODULE_NAME,
        "components_version": _COMPONENTS_VERSION,
    }
    try:
        setattr(pipeline, "metadata_", meta)
    except Exception:
        pass

    # ----------------------------- Save model (joblib) -----------------------------
    print(f"Saving model to: {out_model_path} (joblib compress={joblib_compress})")
    atomic_joblib_dump(pipeline, out_model_path, compress=joblib_compress)

    # ----------------------------- Save NPZ index bundle -----------------------------
    print(f"Writing NPZ bundle: {out_npz_path} (compressed={compressed_npz})")
    atomic_save_npz(
        out_npz_path,
        compressed=compressed_npz,
        verify=True,
        sentence_id=ids.astype(np.int64, copy=False),
        embeddings=Z,
        created_at_utc=np.array(created_at),
        corpus_path=np.array(str(sentences_parquet_path)),
        dataset_fingerprint_sha256=np.array(fingerprint),
        metadata_json=np.array(json.dumps(meta, ensure_ascii=False)),
    )

    with np.load(out_npz_path, allow_pickle=False) as chk:
        if chk["sentence_id"].shape[0] != N or chk["embeddings"].shape[0] != N:
            raise RuntimeError("Post-write verification failed: row counts mismatch.")

    t1 = time.perf_counter()
    print("Done.")
    print(f"  NPZ:   {out_npz_path.resolve()}")
    print(f"  Model: {out_model_path.resolve()}")
    print(f"  Explained variance (sum): {explained_var_sum:.4f}")
    print(f"  Total runtime: {(t1 - t0)/60:.1f} minutes")

    return {
        "out_npz_path": str(out_npz_path),
        "out_model_path": str(out_model_path),
        "n_rows": int(N),
        "dim": int(d),
        "dim_effective": int(effective_dim),
        "use_char_ngrams": bool(chosen_use_char),
        "l2_normalize": bool(config.l2_normalize),
        "explained_variance_ratio_sum": explained_var_sum,
        "dataset_fingerprint_sha256": fingerprint,
        "runtime_seconds": float(t1 - t0),
        "components_module_path": str((Path(__file__).resolve().parent / f"{_COMPONENTS_MODULE_NAME}.py").resolve()),
    }


# ----------------------------- CLI -----------------------------

# ----------------------------- DType parsing -----------------------------

_FLOAT_DTYPE_MAP: Dict[str, np.dtype] = {
    "float16": np.dtype("float16"),
    "float32": np.dtype("float32"),
    "float64": np.dtype("float64"),
}

def _resolve_optional_float_dtype(spec: str | None) -> Optional[np.dtype]:
    """
    Convert a CLI dtype spec into a numpy dtype.

    - "auto"/None => None (do not force a cast; preserve upstream defaults)
    - float16/float32/float64 => corresponding numpy dtype
    """
    if spec is None:
        return None
    s = str(spec).strip().lower()
    if s in ("auto", "none", ""):
        return None
    if s not in _FLOAT_DTYPE_MAP:
        raise ValueError(f"Unsupported dtype '{spec}'. Use one of: auto,float16,float32,float64.")
    return _FLOAT_DTYPE_MAP[s]



def _parse_pair(s: str) -> Tuple[int, int]:
    a, b = s.split(",")
    return int(a.strip()), int(b.strip())


def parse_args(argv) -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Build IDF-SVD embedding index NPZ + save fitted model (joblib).")
    p.add_argument("--corpus", type=Path, default=DEFAULT_CORPUS_PATH, help="Input parquet path.")
    p.add_argument("--out_npz", type=Path, default=DEFAULT_OUT_NPZ, help="Output NPZ path.")
    p.add_argument("--out_model", type=Path, default=DEFAULT_OUT_MODEL, help="Output joblib model path.")
    p.add_argument("--id_col", type=str, default=DEFAULT_ID_COL, help="ID column name.")
    p.add_argument("--text_col", type=str, default=DEFAULT_TEXT_COL, help="Text column name.")

    # Vectorizer knobs
    p.add_argument("--min_df_word", type=int, default=1, help="Word TF-IDF min_df (overrides --min_df).")
    p.add_argument("--max_df_word", type=float, default=0.99, help="Word TF-IDF max_df (overrides --max_df).")
    p.add_argument("--min_df_char", type=int, default=1, help="Char TF-IDF min_df (overrides --min_df).")
    p.add_argument("--max_df_char", type=float, default=0.99, help="Char TF-IDF max_df (overrides --max_df).")
    p.add_argument("--max_features_word", type=int, default=600_000)
    p.add_argument("--max_features_char", type=int, default=300_000)
    p.add_argument("--word_ngrams", type=str, default="1,3", help="Word ngram_range as 'min,max' (e.g., 1,3).")
    p.add_argument("--char_ngrams", type=str, default="3,6", help="Char ngram_range as 'min,max' (e.g., 3,6).")
    p.add_argument("--no_char_ngrams", action="store_true", help="Disable char n-grams.")
    # Branch weighting / preprocessing knobs
    p.add_argument("--transformer_weight_word", type=float, default=1.0, help="FeatureUnion weight for word TF-IDF branch.")
    p.add_argument("--transformer_weight_char", type=float, default=1.0, help="FeatureUnion weight for char TF-IDF branch (often downweighted).")
    p.add_argument("--number_mode", type=str, default="replace", choices=["replace", "keep", "remove"],
                   help="How to handle standalone numeric tokens in preprocessing (default: replace -> num12).")
    p.add_argument("--number_prefix", type=str, default="num",
                   help="Prefix used when --number_mode=replace (e.g., num12).")
    p.add_argument("--no_sublinear_tf", action="store_true", help="Disable sublinear tf scaling.")


    # Numeric precision / parallelism
    p.add_argument(
        "--tfidf_dtype",
        type=str,
        default="float32",
        choices=["auto", "float16", "float32", "float64"],
        help="TF-IDF dtype. Default 'auto' preserves scikit-learn default (typically float64).",
    )
    p.add_argument(
        "--embeddings_dtype",
        type=str,
        default="float32",
        choices=["auto", "float16", "float32", "float64"],
        help="Embeddings dtype written to NPZ. Default 'float32' saves space; 'auto' preserves pipeline output dtype.",
    )
    p.add_argument(
        "--union_n_jobs",
        type=int,
        default=1,
        help="FeatureUnion parallel jobs for word/char branches (0=auto; 1=disable parallelism).",
    )

    # SVD / normalization
    p.add_argument("--dim", type=int, default=1024, help="SVD dimension.")
    p.add_argument("--svd_n_iter", type=int, default=20)
    p.add_argument("--no_l2", action="store_true", help="Disable L2 normalization after SVD.")

    # Output settings
    p.add_argument("--no_compress_npz", action="store_true", help="Disable NPZ compression.")
    p.add_argument("--joblib_compress", type=int, default=3, help="joblib compression level (0-9).")
    p.add_argument("--no_fingerprint", action="store_true", help="Disable dataset fingerprint computation.")

    return p.parse_args(argv)


def main(argv=None) -> int:
    args = parse_args(sys.argv[1:] if argv is None else argv)

    cfg = IDFSVDConfig(
        word_ngram_range=_parse_pair(args.word_ngrams),
        char_ngram_range=_parse_pair(args.char_ngrams),
        use_char_ngrams=not args.no_char_ngrams,
        # NOTE: args.min_df / args.max_df do not exist (historical leftovers). The *_word/_char args are always present.
        min_df_word=int(args.min_df_word),
        max_df_word=float(args.max_df_word),
        min_df_char=int(args.min_df_char),
        max_df_char=float(args.max_df_char),
        max_features_word=args.max_features_word,
        max_features_char=args.max_features_char,
        transformer_weight_word=args.transformer_weight_word,
        transformer_weight_char=args.transformer_weight_char,
        number_mode=args.number_mode,
        number_prefix=args.number_prefix,
        sublinear_tf=not args.no_sublinear_tf,
        dim=args.dim,
        svd_n_iter=args.svd_n_iter,
        random_state=42,
        l2_normalize=not args.no_l2,
    )

    res = build_embedding_index_idf_svd_npz_and_model(
        sentences_parquet_path=args.corpus,
        out_npz_path=args.out_npz,
        out_model_path=args.out_model,
        sentence_id_col=args.id_col,
        text_col=args.text_col,
        compressed_npz=not args.no_compress_npz,
        joblib_compress=int(args.joblib_compress),
        compute_fingerprint_flag=not args.no_fingerprint,
        config=cfg,
        tfidf_dtype=args.tfidf_dtype,
        embeddings_dtype=args.embeddings_dtype,
        union_n_jobs=int(args.union_n_jobs),
    )

    print("\nResult:")
    for k, v in res.items():
        print(f"  {k}: {v}")

    print("\nRetrieval usage (example):")
    print("  import joblib, numpy as np")
    print("  # Ensure project folder is on sys.path if loading from elsewhere:")
    print("  # import sys; sys.path.insert(0, '/path/to/project')")
    print(f"  pipe = joblib.load(r'{args.out_model}')")
    print(f"  idx = np.load(r'{args.out_npz}', allow_pickle=False)")
    print("  emb = idx['embeddings']")
    print("  q_emb = pipe.transform(['query text here'])")
    print("  q_emb = q_emb.astype(emb.dtype, copy=False)  # optional: match index dtype")
    print("  sims = emb @ q_emb[0]  # if l2-normalized, this equals cosine similarity")
    print("  topk = sims.argsort()[-10:][::-1]")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
