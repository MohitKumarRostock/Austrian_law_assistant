#!/usr/bin/env python3
"""
build_embedding_index_idf_svd_npz_mxbai_fixed_vocab.py

This script is the same IDF-SVD (TF-IDF -> TruncatedSVD/LSA -> optional L2 normalization)
pipeline as build_embedding_index_idf_svd_npz.py, with one deliberate change:

  - The *word* TF-IDF branch is forced to use a fixed vocabulary equal to the full
    Hugging Face tokenizer vocabulary of:

        mixedbread-ai/deepset-mxbai-embed-de-large-v1

To do this robustly with portable joblib serialization, the word vectorizer uses a
picklable tokenizer callable (HFTokenizerCallable) defined in an auto-generated
importable module (idf_svd_components_mxbai_fixed_vocab.py), similar to the
original script's permanent serialization fix.

Outputs:
  1) embedding_index_idf_svd.npz
  2) idf_svd_model.joblib

Dependencies:
  python -m pip install -U scikit-learn scipy pyarrow joblib transformers sentencepiece

Notes / trade-offs:
  - Forcing the full tokenizer vocabulary can dramatically increase feature-space width
    and the size of the saved joblib model. This is expected.
  - min_df/max_df/max_features for the word branch are effectively ignored when a fixed
    vocabulary is provided (scikit-learn behavior).

Run:
  python build_embedding_index_idf_svd_npz_mxbai_fixed_vocab.py \
    --dim 512 \
    --no_char_ngrams
"""

from __future__ import annotations

import argparse
import hashlib
import json
import os
import sys
import tempfile
import time
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Optional, Tuple, cast

import numpy as np
import pandas as pd


# ----------------------------- Defaults -----------------------------

DEFAULT_CORPUS_PATH = Path("ris_sentences.parquet")
DEFAULT_OUT_NPZ = Path("embedding_index_idf_svd.npz")
DEFAULT_OUT_MODEL = Path("idf_svd_model.joblib")

DEFAULT_ID_COL = "sentence_id"
DEFAULT_TEXT_COL = "sentence"

# Fixed tokenizer model
DEFAULT_HF_TOKENIZER_MODEL = "mixedbread-ai/deepset-mxbai-embed-de-large-v1"


# ----------------------------- Permanent fix: write/import components module -----------------------------

_COMPONENTS_MODULE_NAME = "idf_svd_components_mxbai_fixed_vocab"
_COMPONENTS_VERSION = "1"

_COMPONENTS_SOURCE = r'''# Auto-generated by build_embedding_index_idf_svd_npz_mxbai_fixed_vocab.py
# Do not edit unless you know what you're doing.
__COMPONENTS_VERSION__ = "__COMPONENTS_VERSION_PLACEHOLDER__"

import re
from typing import Optional

import numpy as np

try:
    from sklearn.base import BaseEstimator, TransformerMixin
except Exception:
    BaseEstimator = object
    TransformerMixin = object

_PARAGRAPH_RE = re.compile(r"ยง+")
_DIGIT_TOKEN_RE = re.compile(r"\b\d+\b")
_MULTI_SPACE_RE = re.compile(r"\s+")


def preprocess_legal_german(text: str, *, number_mode: str = "replace", number_prefix: str = "num") -> str:
    """Pragmatic preprocessing for German legal sentences."""
    s = str(text)
    s = _PARAGRAPH_RE.sub(" paragraf ", s)

    mode = (number_mode or "replace").strip().lower()
    if mode == "remove":
        s = _DIGIT_TOKEN_RE.sub(" ", s)
    elif mode == "replace":
        prefix = (number_prefix or "num").strip() or "num"
        s = _DIGIT_TOKEN_RE.sub(lambda m: f" {prefix}{m.group(0)} ", s)
    elif mode == "keep":
        pass
    else:
        raise ValueError(f"Unsupported number_mode: {number_mode!r} (expected 'replace', 'keep', or 'remove').")

    s = s.lower()
    s = _MULTI_SPACE_RE.sub(" ", s).strip()
    return s


class PreprocessTransformer(BaseEstimator, TransformerMixin):
    """scikit-learn compatible transformer applying preprocess_legal_german."""

    def __init__(self, number_mode: str = "replace", number_prefix: str = "num"):
        self.number_mode = number_mode
        self.number_prefix = number_prefix

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [
            preprocess_legal_german(x, number_mode=self.number_mode, number_prefix=self.number_prefix)
            for x in X
        ]


class AdaptiveTruncatedSVD(BaseEstimator, TransformerMixin):
    """TruncatedSVD wrapper that clips n_components at fit-time based on X shape."""

    def __init__(
        self,
        n_components: int = 512,
        *,
        n_iter: int = 20,
        random_state: int = 42,
        algorithm: str = "randomized",
    ):
        self.n_components = int(n_components)
        self.n_iter = int(n_iter)
        self.random_state = int(random_state)
        self.algorithm = str(algorithm)

        self.n_components_: Optional[int] = None
        self.model_ = None
        self.explained_variance_ratio_ = None
        self.components_ = None

    def fit(self, X, y=None):
        from sklearn.decomposition import TruncatedSVD

        n_samples = int(getattr(X, "shape", [0, 0])[0])
        n_features = int(getattr(X, "shape", [0, 0])[1])

        max_allowed = min(n_samples, n_features) - 1
        if max_allowed < 1:
            raise ValueError("Cannot fit TruncatedSVD: shape=({},{})".format(n_samples, n_features))

        n_comp = min(int(self.n_components), int(max_allowed))

        self.n_components_ = int(n_comp)
        self.model_ = TruncatedSVD(
            n_components=self.n_components_,
            algorithm=self.algorithm,
            n_iter=self.n_iter,
            random_state=self.random_state,
        )
        self.model_.fit(X)

        self.explained_variance_ratio_ = getattr(self.model_, "explained_variance_ratio_", None)
        self.components_ = getattr(self.model_, "components_", None)
        return self

    def transform(self, X):
        if self.model_ is None:
            raise RuntimeError("AdaptiveTruncatedSVD is not fitted.")
        return self.model_.transform(X)

    def fit_transform(self, X, y=None):
        return self.fit(X, y=y).transform(X)


class HFTokenizerCallable:
    """Picklable callable wrapper for Hugging Face tokenization.

    Used as TfidfVectorizer(tokenizer=HFTokenizerCallable(...)).

    The internal tokenizer object is created lazily and is not persisted in the pickle.
    """

    def __init__(
        self,
        model_name: str,
        *,
        use_fast: bool = True,
        max_length: Optional[int] = None,
    ):
        self.model_name = str(model_name)
        self.use_fast = bool(use_fast)
        self.max_length = int(max_length) if max_length is not None else None
        self._tok = None

    def _ensure_tok(self):
        if self._tok is None:
            from transformers import AutoTokenizer

            self._tok = AutoTokenizer.from_pretrained(self.model_name, use_fast=self.use_fast)
            try:
                self._tok.model_max_length = 10**12
            except Exception:
                pass
    def __call__(self, text: str):
        self._ensure_tok()
        toks = self._tok.tokenize(str(text))
        if self.max_length is not None and self.max_length > 0:
            toks = toks[: self.max_length]
        return toks

    def __getstate__(self):
        # Do not pickle the HF tokenizer object.
        return {
            "model_name": self.model_name,
            "use_fast": self.use_fast,
            "max_length": self.max_length,
        }

    def __setstate__(self, state):
        self.model_name = state.get("model_name")
        self.use_fast = bool(state.get("use_fast", True))
        ml = state.get("max_length", None)
        self.max_length = int(ml) if ml is not None else None
        self._tok = None
'''
_COMPONENTS_SOURCE = _COMPONENTS_SOURCE.replace("__COMPONENTS_VERSION_PLACEHOLDER__", _COMPONENTS_VERSION)


def _atomic_write_text(path: Path, text: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path: Optional[Path] = None
    try:
        with tempfile.NamedTemporaryFile(
            mode="w",
            encoding="utf-8",
            dir=str(path.parent),
            prefix=path.stem + ".tmp.",
            suffix=path.suffix if path.suffix else ".py",
            delete=False,
        ) as f:
            tmp_path = Path(f.name)
            f.write(text)
        os.replace(tmp_path, path)
    except Exception:
        if tmp_path is not None and tmp_path.exists():
            try:
                tmp_path.unlink()
            except Exception:
                pass
        raise


def ensure_components_module(script_dir: Path) -> Path:
    mod_path = script_dir / f"{_COMPONENTS_MODULE_NAME}.py"
    try:
        existing = mod_path.read_text(encoding="utf-8", errors="replace") if mod_path.exists() else None
    except Exception:
        existing = None

    needs_write = (existing != _COMPONENTS_SOURCE)
    if needs_write:
        _atomic_write_text(mod_path, _COMPONENTS_SOURCE)

    return mod_path


def import_components(script_dir: Path):
    if str(script_dir) not in sys.path:
        sys.path.insert(0, str(script_dir))

    import importlib

    mod = importlib.import_module(_COMPONENTS_MODULE_NAME)
    ver = getattr(mod, "__COMPONENTS_VERSION__", None)
    if ver != _COMPONENTS_VERSION:
        raise RuntimeError(
            f"Imported {_COMPONENTS_MODULE_NAME} version {ver}, expected {_COMPONENTS_VERSION}. "
            f"Delete {_COMPONENTS_MODULE_NAME}.py and rerun the builder."
        )
    return mod


# ----------------------------- Integrity fingerprint -----------------------------


def compute_dataset_fingerprint(
    df: pd.DataFrame,
    *,
    id_col: str,
    text_col: str,
    include_cols: Tuple[str, ...] = (),
) -> str:
    if include_cols:
        raise ValueError(
            "This fingerprint function is defined to be equivalent to "
            "sha256_fingerprint_sentence_id_content and therefore does not "
            "support include_cols. Pass include_cols=() to proceed."
        )

    if id_col not in df.columns or text_col not in df.columns:
        raise ValueError(
            f"Corpus is missing required column '{id_col}' or '{text_col}'. "
            f"Columns: {list(df.columns)}"
        )

    h = hashlib.sha256()
    for sid, content in df[[id_col, text_col]].itertuples(index=False, name=None):
        h.update(str(sid).encode("utf-8"))
        h.update(b"\0")
        h.update(str(content).encode("utf-8"))
        h.update(b"\n")

    return h.hexdigest()


# ----------------------------- Atomic writers -----------------------------


def atomic_save_npz(out_path: Path, *, compressed: bool, verify: bool = True, **payload: Any) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path: Optional[Path] = None
    try:
        with tempfile.NamedTemporaryFile(
            mode="wb",
            dir=str(out_path.parent),
            prefix=out_path.stem + ".tmp.",
            suffix=".npz",
            delete=False,
        ) as f:
            tmp_path = Path(f.name)
            if compressed:
                np.savez_compressed(f, **payload)
            else:
                np.savez(f, **payload)

        if verify:
            with np.load(tmp_path, allow_pickle=False) as d:
                if "sentence_id" not in d or "embeddings" not in d:
                    raise RuntimeError("NPZ verification failed: missing required keys.")
                if d["sentence_id"].ndim != 1 or d["embeddings"].ndim != 2:
                    raise RuntimeError("NPZ verification failed: unexpected shapes.")

        os.replace(tmp_path, out_path)
    except Exception:
        if tmp_path is not None and tmp_path.exists():
            try:
                tmp_path.unlink()
            except Exception:
                pass
        raise


def atomic_joblib_dump(obj: Any, out_path: Path, *, compress: int = 3) -> None:
    import joblib

    out_path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path: Optional[Path] = None
    try:
        with tempfile.NamedTemporaryFile(
            mode="wb",
            dir=str(out_path.parent),
            prefix=out_path.stem + ".tmp.",
            suffix=out_path.suffix if out_path.suffix else ".joblib",
            delete=False,
        ) as f:
            tmp_path = Path(f.name)

        joblib.dump(obj, str(tmp_path), compress=compress)
        _ = joblib.load(str(tmp_path))

        os.replace(tmp_path, out_path)
    except Exception:
        if tmp_path is not None and tmp_path.exists():
            try:
                tmp_path.unlink()
            except Exception:
                pass
        raise


# ----------------------------- Config -----------------------------


@dataclass(frozen=True)
class IDFSVDConfig:
    # Vectorization
    word_ngram_range: Tuple[int, int] = (1, 1)  # kept for interface; ngrams beyond 1 are not meaningful for HF subwords
    char_ngram_range: Tuple[int, int] = (3, 6)
    use_char_ngrams: bool = True

    # Document frequency thresholds
    min_df_word: int = 1
    max_df_word: float = 1.0
    min_df_char: int = 1
    max_df_char: float = 0.99

    max_features_word: int = 0  # ignored with fixed vocabulary
    max_features_char: int = 300_000

    transformer_weight_word: float = 1.0
    transformer_weight_char: float = 1.0

    number_mode: str = "keep"  # replace|keep|remove
    number_prefix: str = "num"

    sublinear_tf: bool = False

    # SVD
    dim: int = 512
    svd_n_iter: int = 20
    random_state: int = 42

    # Postprocessing
    l2_normalize: bool = False

    # HF tokenization
    hf_tokenizer_model: str = DEFAULT_HF_TOKENIZER_MODEL
    hf_use_fast: bool = True
    hf_max_length: Optional[int] = None


# ----------------------------- HF vocab -----------------------------


def load_full_tokenizer_vocab(model_name: str, *, use_fast: bool = True) -> Dict[str, int]:
    """Return a contiguous (0..V-1) token->index dict built from HF tokenizer vocab.

    We preserve HF's token-id order by sorting by the tokenizer ids first, then
    remapping to a contiguous index space to satisfy scikit-learn vocabulary constraints.
    """
    from transformers import AutoTokenizer

    tok = AutoTokenizer.from_pretrained(model_name, use_fast=use_fast)
    v = tok.get_vocab()  # token -> id
    # deterministic order
    items = sorted(v.items(), key=lambda kv: kv[1])
    return {t: i for i, (t, _tid) in enumerate(items)}


def build_idf_svd_pipeline(
    *,
    config: IDFSVDConfig,
    use_char_ngrams: bool,
    dim: int,
    components_mod,
    fixed_vocab: Dict[str, int],
    tfidf_dtype: Optional[np.dtype] = None,
    union_n_jobs: Optional[int] = None,
):
    """Return a scikit-learn Pipeline for IDF-SVD embeddings with fixed HF vocab."""
    from sklearn.base import TransformerMixin
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.pipeline import Pipeline, FeatureUnion
    from sklearn.preprocessing import Normalizer

    PreprocessTransformer = components_mod.PreprocessTransformer
    AdaptiveTruncatedSVD = components_mod.AdaptiveTruncatedSVD
    HFTokenizerCallable = components_mod.HFTokenizerCallable

    # Word TF-IDF with fixed vocabulary and HF tokenizer
    word_params: Dict[str, Any] = dict(
        analyzer="word",
        ngram_range=config.word_ngram_range,
        min_df=config.min_df_word,
        max_df=config.max_df_word,
        vocabulary=fixed_vocab,
        tokenizer=HFTokenizerCallable(
            config.hf_tokenizer_model,
            use_fast=config.hf_use_fast,
            max_length=config.hf_max_length,
        ),
        preprocessor=None,
        token_pattern=None,
        sublinear_tf=config.sublinear_tf,
        lowercase=False,
        norm=None,
    )
    if tfidf_dtype is not None:
        word_params["dtype"] = getattr(tfidf_dtype, "type", tfidf_dtype)

    word_vectorizer = TfidfVectorizer(**word_params)

    transformer_list: list[tuple[str, TransformerMixin | Pipeline]] = [
        ("word_tfidf", cast(TransformerMixin, word_vectorizer)),
    ]

    if use_char_ngrams:
        char_params: Dict[str, Any] = dict(
            analyzer="char_wb",
            ngram_range=config.char_ngram_range,
            min_df=config.min_df_char,
            max_df=config.max_df_char,
            max_features=config.max_features_char,
            sublinear_tf=config.sublinear_tf,
            lowercase=False,
            norm=None,
        )
        if tfidf_dtype is not None:
            char_params["dtype"] = getattr(tfidf_dtype, "type", tfidf_dtype)
        char_vectorizer = TfidfVectorizer(**char_params)
        transformer_list.append(("char_tfidf", cast(TransformerMixin, char_vectorizer)))

    fu_kwargs: Dict[str, Any] = {}
    if union_n_jobs is not None:
        fu_kwargs["n_jobs"] = int(union_n_jobs)

    weights = {"word_tfidf": float(config.transformer_weight_word)}
    if use_char_ngrams:
        weights["char_tfidf"] = float(config.transformer_weight_char)
    fu_kwargs["transformer_weights"] = weights

    try:
        union = FeatureUnion(transformer_list=transformer_list, **fu_kwargs)
    except TypeError:
        fu_kwargs.pop("transformer_weights", None)
        try:
            union = FeatureUnion(transformer_list=transformer_list, **fu_kwargs)
        except TypeError:
            union = FeatureUnion(transformer_list, **fu_kwargs)

    steps = [
        ("preprocess", PreprocessTransformer(number_mode=config.number_mode, number_prefix=config.number_prefix)),
        ("tfidf_union", union),
        (
            "svd",
            AdaptiveTruncatedSVD(
                n_components=int(dim),
                algorithm="randomized",
                n_iter=config.svd_n_iter,
                random_state=config.random_state,
            ),
        ),
    ]

    if config.l2_normalize:
        steps.append(("l2norm", Normalizer(copy=False)))

    return Pipeline(steps=steps)


# ----------------------------- Builder (NPZ + model) -----------------------------


def build_embedding_index_idf_svd_npz_and_model(
    *,
    sentences_parquet_path: str | Path,
    out_npz_path: str | Path,
    out_model_path: str | Path,
    sentence_id_col: str = DEFAULT_ID_COL,
    text_col: str = DEFAULT_TEXT_COL,
    compressed_npz: bool = True,
    joblib_compress: int = 3,
    compute_fingerprint_flag: bool = True,
    config: IDFSVDConfig = IDFSVDConfig(),
    tfidf_dtype: str = "float32",
    embeddings_dtype: str = "float32",
    union_n_jobs: int = 1,
) -> Dict[str, Any]:
    import sklearn

    t0 = time.perf_counter()

    tfidf_np_dtype = _resolve_optional_float_dtype(tfidf_dtype)
    emb_np_dtype = _resolve_optional_float_dtype(embeddings_dtype)

    sentences_parquet_path = Path(sentences_parquet_path)
    out_npz_path = Path(out_npz_path)
    out_model_path = Path(out_model_path)

    script_dir = Path(__file__).resolve().parent
    ensure_components_module(script_dir)
    components_mod = import_components(script_dir)

    print(f"Loading corpus: {sentences_parquet_path}")
    try:
        df = pd.read_parquet(sentences_parquet_path, columns=[sentence_id_col, text_col])
    except TypeError:
        df = pd.read_parquet(sentences_parquet_path)

    if compute_fingerprint_flag:
        for c in ("law_type", "page"):
            if c in df.columns:
                continue
            try:
                extra = pd.read_parquet(sentences_parquet_path, columns=[c])
                if c in extra.columns and len(extra) == len(df):
                    df[c] = extra[c]
            except Exception:
                pass

    if sentence_id_col not in df.columns:
        raise ValueError(f"Missing '{sentence_id_col}' in parquet. Columns: {list(df.columns)}")
    if text_col not in df.columns:
        raise ValueError(f"Missing '{text_col}' in parquet. Columns: {list(df.columns)}")

    ids = df[sentence_id_col].astype(np.int64).to_numpy()
    if len(np.unique(ids)) != len(ids):
        raise ValueError(f"'{sentence_id_col}' must be unique (duplicates found).")

    texts_raw = df[text_col].astype(str).tolist()
    N = len(texts_raw)
    if N == 0:
        raise ValueError("No sentences found (N=0).")
    print(f"Number of sentences: {N:,}")

    fingerprint = ""
    if compute_fingerprint_flag:
        print("Computing dataset fingerprint (sha256) ...")
        fp_start = time.perf_counter()
        fingerprint = compute_dataset_fingerprint(df, id_col=sentence_id_col, text_col=text_col)
        fp_end = time.perf_counter()
        print(f"Fingerprint computed in {fp_end - fp_start:.1f} s: {fingerprint}")

    chosen_dim = int(config.dim)
    chosen_use_char = bool(config.use_char_ngrams)
    print(f"Using fixed settings: dim={chosen_dim}, use_char_ngrams={chosen_use_char}")

    # Load fixed HF vocabulary
    print(f"Loading full HF tokenizer vocab: {config.hf_tokenizer_model} (use_fast={config.hf_use_fast})")
    vocab_start = time.perf_counter()
    fixed_vocab = load_full_tokenizer_vocab(config.hf_tokenizer_model, use_fast=config.hf_use_fast)
    vocab_end = time.perf_counter()
    print(f"HF vocabulary size: {len(fixed_vocab):,} (loaded in {vocab_end - vocab_start:.1f} s)")

    print("Fitting final IDF-SVD pipeline on full corpus ...")

    if union_n_jobs == 0:
        cpu = os.cpu_count() or 1
        union_n_jobs_eff = 2 if (chosen_use_char and cpu >= 2) else 1
    else:
        union_n_jobs_eff = int(union_n_jobs)
    union_n_jobs_eff = max(1, union_n_jobs_eff)
    union_n_jobs_eff = None if union_n_jobs_eff == 1 else union_n_jobs_eff

    pipeline = build_idf_svd_pipeline(
        config=config,
        use_char_ngrams=chosen_use_char,
        dim=chosen_dim,
        components_mod=components_mod,
        fixed_vocab=fixed_vocab,
        tfidf_dtype=tfidf_np_dtype,
        union_n_jobs=union_n_jobs_eff,
    )

    Z = pipeline.fit_transform(texts_raw)
    Z = np.asarray(Z)
    if emb_np_dtype is not None and Z.dtype != emb_np_dtype:
        Z = Z.astype(emb_np_dtype, copy=False)

    if Z.ndim != 2 or Z.shape[0] != N:
        raise RuntimeError(f"Unexpected embedding matrix shape: {Z.shape} (expected ({N}, d))")

    d = int(Z.shape[1])
    print(f"Embeddings shape: {Z.shape} ({Z.dtype})")

    svd_step = pipeline.named_steps["svd"]
    evr = getattr(svd_step, "explained_variance_ratio_", None)
    explained_var_sum = float(np.sum(evr)) if evr is not None else 0.0
    effective_dim = int(getattr(svd_step, "n_components_", d) or d)

    created_at = datetime.now(timezone.utc).isoformat()

    meta = {
        "created_at_utc": created_at,
        "method": "tfidf+svd (lsa)",
        "tfidf_dtype": (np.dtype(tfidf_np_dtype).name if tfidf_np_dtype is not None else "auto"),
        "embeddings_dtype": str(Z.dtype),
        "union_n_jobs": int(union_n_jobs_eff or 1),
        "chosen_dim_requested": int(chosen_dim),
        "chosen_dim_effective": int(effective_dim),
        "use_char_ngrams": bool(chosen_use_char),
        "vectorizer": {
            "word_ngram_range": list(config.word_ngram_range),
            "char_ngram_range": list(config.char_ngram_range),
            "min_df_word": int(config.min_df_word),
            "max_df_word": float(config.max_df_word),
            "min_df_char": int(config.min_df_char),
            "max_df_char": float(config.max_df_char),
            "transformer_weight_word": float(config.transformer_weight_word),
            "transformer_weight_char": float(config.transformer_weight_char),
            "number_mode": str(config.number_mode),
            "number_prefix": str(config.number_prefix),
            "max_features_word": int(config.max_features_word),
            "max_features_char": int(config.max_features_char),
            "sublinear_tf": bool(config.sublinear_tf),
            "fixed_vocab": {
                "model": str(config.hf_tokenizer_model),
                "vocab_size": int(len(fixed_vocab)),
                "use_fast": bool(config.hf_use_fast),
                "max_length": config.hf_max_length,
            },
        },
        "postprocess": {"l2_normalize": bool(config.l2_normalize)},
        "svd": {"explained_variance_ratio_sum": explained_var_sum, "n_iter": int(config.svd_n_iter)},
        "environment": {"numpy": np.__version__, "pandas": pd.__version__, "sklearn": sklearn.__version__},
        "dataset_fingerprint_sha256": fingerprint,
        "corpus_path": str(sentences_parquet_path),
        "components_module": _COMPONENTS_MODULE_NAME,
        "components_version": _COMPONENTS_VERSION,
    }
    try:
        setattr(pipeline, "metadata_", meta)
    except Exception:
        pass

    print(f"Saving model to: {out_model_path} (joblib compress={joblib_compress})")
    atomic_joblib_dump(pipeline, out_model_path, compress=joblib_compress)

    print(f"Writing NPZ bundle: {out_npz_path} (compressed={compressed_npz})")
    atomic_save_npz(
        out_npz_path,
        compressed=compressed_npz,
        verify=True,
        sentence_id=ids.astype(np.int64, copy=False),
        embeddings=Z,
        created_at_utc=np.array(created_at),
        corpus_path=np.array(str(sentences_parquet_path)),
        dataset_fingerprint_sha256=np.array(fingerprint),
        metadata_json=np.array(json.dumps(meta, ensure_ascii=False)),
    )

    with np.load(out_npz_path, allow_pickle=False) as chk:
        if chk["sentence_id"].shape[0] != N or chk["embeddings"].shape[0] != N:
            raise RuntimeError("Post-write verification failed: row counts mismatch.")

    t1 = time.perf_counter()
    print("Done.")
    print(f"  NPZ:   {out_npz_path.resolve()}")
    print(f"  Model: {out_model_path.resolve()}")
    print(f"  Explained variance (sum): {explained_var_sum:.4f}")
    print(f"  Total runtime: {(t1 - t0)/60:.1f} minutes")

    return {
        "out_npz_path": str(out_npz_path),
        "out_model_path": str(out_model_path),
        "n_rows": int(N),
        "dim": int(d),
        "dim_effective": int(effective_dim),
        "use_char_ngrams": bool(chosen_use_char),
        "l2_normalize": bool(config.l2_normalize),
        "explained_variance_ratio_sum": explained_var_sum,
        "dataset_fingerprint_sha256": fingerprint,
        "runtime_seconds": float(t1 - t0),
        "components_module_path": str((Path(__file__).resolve().parent / f"{_COMPONENTS_MODULE_NAME}.py").resolve()),
        "hf_vocab_size": int(len(fixed_vocab)),
    }


# ----------------------------- CLI -----------------------------


_FLOAT_DTYPE_MAP: Dict[str, np.dtype] = {
    "float16": np.dtype("float16"),
    "float32": np.dtype("float32"),
    "float64": np.dtype("float64"),
}


def _resolve_optional_float_dtype(spec: str | None) -> Optional[np.dtype]:
    if spec is None:
        return None
    s = str(spec).strip().lower()
    if s in ("auto", "none", ""):
        return None
    if s not in _FLOAT_DTYPE_MAP:
        raise ValueError(f"Unsupported dtype '{spec}'. Use one of: auto,float16,float32,float64.")
    return _FLOAT_DTYPE_MAP[s]


def _parse_pair(s: str) -> Tuple[int, int]:
    a, b = s.split(",")
    return int(a.strip()), int(b.strip())


def parse_args(argv) -> argparse.Namespace:
    p = argparse.ArgumentParser(
        description="Build IDF-SVD embedding index NPZ + save fitted model (joblib) with fixed MXBAI tokenizer vocab."
    )
    p.add_argument("--corpus", type=Path, default=DEFAULT_CORPUS_PATH, help="Input parquet path.")
    p.add_argument("--out_npz", type=Path, default=DEFAULT_OUT_NPZ, help="Output NPZ path.")
    p.add_argument("--out_model", type=Path, default=DEFAULT_OUT_MODEL, help="Output joblib model path.")
    p.add_argument("--id_col", type=str, default=DEFAULT_ID_COL, help="ID column name.")
    p.add_argument("--text_col", type=str, default=DEFAULT_TEXT_COL, help="Text column name.")

    # Vectorizer knobs
    p.add_argument("--min_df_word", type=int, default=1, help="Word TF-IDF min_df (kept for interface; fixed vocab).")
    p.add_argument("--max_df_word", type=float, default=1.0, help="Word TF-IDF max_df (kept for interface; fixed vocab).")
    p.add_argument("--min_df_char", type=int, default=1, help="Char TF-IDF min_df.")
    p.add_argument("--max_df_char", type=float, default=0.99, help="Char TF-IDF max_df.")
    p.add_argument("--max_features_word", type=int, default=0, help="Ignored for word branch when fixed vocab is used.")
    p.add_argument("--max_features_char", type=int, default=300_000)
    p.add_argument("--word_ngrams", type=str, default="1,1", help="Word ngram_range as 'min,max'.")
    p.add_argument("--char_ngrams", type=str, default="3,6", help="Char ngram_range as 'min,max'.")
    p.add_argument("--no_char_ngrams", action="store_true", help="Disable char n-grams.")

    p.add_argument("--transformer_weight_word", type=float, default=1.0)
    p.add_argument("--transformer_weight_char", type=float, default=1.0)

    p.add_argument(
        "--number_mode",
        type=str,
        default="replace",
        choices=["replace", "keep", "remove"],
        help="How to handle standalone numeric tokens in preprocessing (default: replace -> num12).",
    )
    p.add_argument("--number_prefix", type=str, default="num")
    p.add_argument("--no_sublinear_tf", action="store_true", help="Disable sublinear tf scaling.")

    # HF tokenization (model is fixed by default; you may override for experimentation)
    p.add_argument(
        "--hf_tokenizer_model",
        type=str,
        default=DEFAULT_HF_TOKENIZER_MODEL,
        help="Hugging Face tokenizer model to source the full fixed vocabulary from.",
    )
    p.add_argument("--hf_slow", action="store_true", help="Use slow HF tokenizer (use_fast=False).")
    p.add_argument(
        "--hf_max_length",
        type=int,
        default=0,
        help="Optional max number of tokens per document passed to TF-IDF (0=disabled).",
    )

    # Numeric precision / parallelism
    p.add_argument(
        "--tfidf_dtype",
        type=str,
        default="float32",
        choices=["auto", "float16", "float32", "float64"],
        help="TF-IDF dtype. 'auto' preserves scikit-learn default (typically float64).",
    )
    p.add_argument(
        "--embeddings_dtype",
        type=str,
        default="float32",
        choices=["auto", "float16", "float32", "float64"],
        help="Embeddings dtype written to NPZ.",
    )
    p.add_argument(
        "--union_n_jobs",
        type=int,
        default=1,
        help="FeatureUnion parallel jobs for word/char branches (0=auto; 1=disable parallelism).",
    )

    # SVD / normalization
    p.add_argument("--dim", type=int, default=512, help="SVD dimension.")
    p.add_argument("--svd_n_iter", type=int, default=20)
    p.add_argument("--no_l2", action="store_true", help="Disable L2 normalization after SVD.")

    # Output settings
    p.add_argument("--no_compress_npz", action="store_true", help="Disable NPZ compression.")
    p.add_argument("--joblib_compress", type=int, default=3, help="joblib compression level (0-9).")
    p.add_argument("--no_fingerprint", action="store_true", help="Disable dataset fingerprint computation.")

    return p.parse_args(argv)


def main(argv=None) -> int:
    args = parse_args(sys.argv[1:] if argv is None else argv)

    hf_max_length = None if int(args.hf_max_length) <= 0 else int(args.hf_max_length)

    cfg = IDFSVDConfig(
        word_ngram_range=_parse_pair(args.word_ngrams),
        char_ngram_range=_parse_pair(args.char_ngrams),
        use_char_ngrams=not args.no_char_ngrams,
        min_df_word=int(args.min_df_word),
        max_df_word=float(args.max_df_word),
        min_df_char=int(args.min_df_char),
        max_df_char=float(args.max_df_char),
        max_features_word=int(args.max_features_word),
        max_features_char=int(args.max_features_char),
        transformer_weight_word=float(args.transformer_weight_word),
        transformer_weight_char=float(args.transformer_weight_char),
        number_mode=args.number_mode,
        number_prefix=args.number_prefix,
        sublinear_tf=not args.no_sublinear_tf,
        dim=int(args.dim),
        svd_n_iter=int(args.svd_n_iter),
        random_state=42,
        l2_normalize=not args.no_l2,
        hf_tokenizer_model=str(args.hf_tokenizer_model),
        hf_use_fast=not bool(args.hf_slow),
        hf_max_length=hf_max_length,
    )

    res = build_embedding_index_idf_svd_npz_and_model(
        sentences_parquet_path=args.corpus,
        out_npz_path=args.out_npz,
        out_model_path=args.out_model,
        sentence_id_col=args.id_col,
        text_col=args.text_col,
        compressed_npz=not args.no_compress_npz,
        joblib_compress=int(args.joblib_compress),
        compute_fingerprint_flag=not args.no_fingerprint,
        config=cfg,
        tfidf_dtype=args.tfidf_dtype,
        embeddings_dtype=args.embeddings_dtype,
        union_n_jobs=int(args.union_n_jobs),
    )

    print("\nResult:")
    for k, v in res.items():
        print(f"  {k}: {v}")

    print("\nRetrieval usage (example):")
    print("  import joblib, numpy as np")
    print("  # Ensure project folder is on sys.path if loading from elsewhere:")
    print("  # import sys; sys.path.insert(0, '/path/to/project')")
    print(f"  pipe = joblib.load(r'{args.out_model}')")
    print(f"  idx = np.load(r'{args.out_npz}', allow_pickle=False)")
    print("  emb = idx['embeddings']")
    print("  q_emb = pipe.transform(['query text here'])")
    print("  q_emb = q_emb.astype(emb.dtype, copy=False)  # optional: match index dtype")
    print("  sims = emb @ q_emb[0]  # if l2-normalized, equals cosine similarity")
    print("  topk = sims.argsort()[-10:][::-1]")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
