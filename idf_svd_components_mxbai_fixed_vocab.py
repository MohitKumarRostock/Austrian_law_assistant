# Auto-generated by build_embedding_index_idf_svd_npz_mxbai_fixed_vocab.py
# Do not edit unless you know what you're doing.
__COMPONENTS_VERSION__ = "1"

import re
from typing import Optional

import numpy as np

try:
    from sklearn.base import BaseEstimator, TransformerMixin
except Exception:
    BaseEstimator = object
    TransformerMixin = object

_PARAGRAPH_RE = re.compile(r"ยง+")
_DIGIT_TOKEN_RE = re.compile(r"\b\d+\b")
_MULTI_SPACE_RE = re.compile(r"\s+")


def preprocess_legal_german(text: str, *, number_mode: str = "replace", number_prefix: str = "num") -> str:
    """Pragmatic preprocessing for German legal sentences."""
    s = str(text)
    s = _PARAGRAPH_RE.sub(" paragraf ", s)

    mode = (number_mode or "replace").strip().lower()
    if mode == "remove":
        s = _DIGIT_TOKEN_RE.sub(" ", s)
    elif mode == "replace":
        prefix = (number_prefix or "num").strip() or "num"
        s = _DIGIT_TOKEN_RE.sub(lambda m: f" {prefix}{m.group(0)} ", s)
    elif mode == "keep":
        pass
    else:
        raise ValueError(f"Unsupported number_mode: {number_mode!r} (expected 'replace', 'keep', or 'remove').")

    s = s.lower()
    s = _MULTI_SPACE_RE.sub(" ", s).strip()
    return s


class PreprocessTransformer(BaseEstimator, TransformerMixin):
    """scikit-learn compatible transformer applying preprocess_legal_german."""

    def __init__(self, number_mode: str = "replace", number_prefix: str = "num"):
        self.number_mode = number_mode
        self.number_prefix = number_prefix

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [
            preprocess_legal_german(x, number_mode=self.number_mode, number_prefix=self.number_prefix)
            for x in X
        ]


class AdaptiveTruncatedSVD(BaseEstimator, TransformerMixin):
    """TruncatedSVD wrapper that clips n_components at fit-time based on X shape."""

    def __init__(
        self,
        n_components: int = 512,
        *,
        n_iter: int = 20,
        random_state: int = 42,
        algorithm: str = "randomized",
    ):
        self.n_components = int(n_components)
        self.n_iter = int(n_iter)
        self.random_state = int(random_state)
        self.algorithm = str(algorithm)

        self.n_components_: Optional[int] = None
        self.model_ = None
        self.explained_variance_ratio_ = None
        self.components_ = None

    def fit(self, X, y=None):
        from sklearn.decomposition import TruncatedSVD

        n_samples = int(getattr(X, "shape", [0, 0])[0])
        n_features = int(getattr(X, "shape", [0, 0])[1])

        max_allowed = min(n_samples, n_features) - 1
        if max_allowed < 1:
            raise ValueError("Cannot fit TruncatedSVD: shape=({},{})".format(n_samples, n_features))

        n_comp = min(int(self.n_components), int(max_allowed))

        self.n_components_ = int(n_comp)
        self.model_ = TruncatedSVD(
            n_components=self.n_components_,
            algorithm=self.algorithm,
            n_iter=self.n_iter,
            random_state=self.random_state,
        )
        self.model_.fit(X)

        self.explained_variance_ratio_ = getattr(self.model_, "explained_variance_ratio_", None)
        self.components_ = getattr(self.model_, "components_", None)
        return self

    def transform(self, X):
        if self.model_ is None:
            raise RuntimeError("AdaptiveTruncatedSVD is not fitted.")
        return self.model_.transform(X)

    def fit_transform(self, X, y=None):
        return self.fit(X, y=y).transform(X)


class HFTokenizerCallable:
    """Picklable callable wrapper for Hugging Face tokenization.

    Used as TfidfVectorizer(tokenizer=HFTokenizerCallable(...)).

    The internal tokenizer object is created lazily and is not persisted in the pickle.
    """

    def __init__(
        self,
        model_name: str,
        *,
        use_fast: bool = True,
        max_length: Optional[int] = None,
    ):
        self.model_name = str(model_name)
        self.use_fast = bool(use_fast)
        self.max_length = int(max_length) if max_length is not None else None
        self._tok = None

    def _ensure_tok(self):
        if self._tok is None:
            from transformers import AutoTokenizer

            self._tok = AutoTokenizer.from_pretrained(self.model_name, use_fast=self.use_fast)
            try:
                self._tok.model_max_length = 10**12
            except Exception:
                pass
    def __call__(self, text: str):
        self._ensure_tok()
        toks = self._tok.tokenize(str(text))
        if self.max_length is not None and self.max_length > 0:
            toks = toks[: self.max_length]
        return toks

    def __getstate__(self):
        # Do not pickle the HF tokenizer object.
        return {
            "model_name": self.model_name,
            "use_fast": self.use_fast,
            "max_length": self.max_length,
        }

    def __setstate__(self, state):
        self.model_name = state.get("model_name")
        self.use_fast = bool(state.get("use_fast", True))
        ml = state.get("max_length", None)
        self.max_length = int(ml) if ml is not None else None
        self._tok = None
